{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "9ec1b5ff",
            "metadata": {},
            "source": [
                "# _Decision trees_\n",
                "\n",
                "Patroonherkenning in gestructureerde (en in bepaalde gevallen ook semi-gestructureerde) data is vaak het meest succesvol met zogenaamde _decision tree_ modellen, of althans een variant daarvan.\n",
                "\n",
                "## Model\n",
                "Het model bestaat uit een **beslissingsboom**. **Iedere vertakking staat voor een bepaalde feature die opgesplitst wordt**.\n",
                "\n",
                ":::{note} üåç\n",
                ":icon: false\n",
                ":class: simple \n",
                "We illustreren dit aan de hand van de bekende [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
                "  \n",
                "De data is afkomstig van drie varianten van Irisbloemen:\n",
                "1. Iris setosa (_target feature: 0_) \n",
                "2. Iris versicolor (_target feature: 1_) \n",
                "3. Iris virginica (_target feature: 2_)  \n",
                "  \n",
                "Voor elke soort werden 50 observaties gemaakt van vier continue features: \n",
                "1. Sepal length (cm) (_feature kolom: 0_)\n",
                "2. Sepal width (cm) (_feature kolom: 1_)\n",
                "3. Petal length (cm) (_feature kolom: 2_)\n",
                "4. Petal width (cm) (_feature kolom: 3_) \n",
                "  \n",
                "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Petal-sepal.jpg/250px-Petal-sepal.jpg)\n",
                ":::"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a12f85bf",
            "metadata": {},
            "outputs": [],
            "source": [
                "import dtreeviz\n",
                "import matplotlib\n",
                "import pandas as pd\n",
                "from sklearn import tree\n",
                "from sklearn.datasets import load_iris\n",
                "\n",
                "matplotlib.set_loglevel(\"ERROR\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dd0d7a0b",
            "metadata": {},
            "outputs": [],
            "source": [
                "iris = load_iris()\n",
                "X, y = iris.data, iris.target\n",
                "clf = tree.DecisionTreeClassifier(random_state=123)\n",
                "clf = clf.fit(X, y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9ca626d9",
            "metadata": {},
            "outputs": [],
            "source": [
                "viz_model = dtreeviz.model(\n",
                "    clf,\n",
                "    X_train=X,\n",
                "    y_train=y,\n",
                "    feature_names=iris.feature_names,\n",
                "    target_name=\"Iris type\",\n",
                "    class_names=iris.target_names,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "59f10bcc",
            "metadata": {},
            "outputs": [],
            "source": [
                "viz_model.view(orientation=\"LR\", fontname=\"DejaVu Sans Mono\", scale=1.5)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "edcca73f",
            "metadata": {},
            "source": [
                ":::{note} üåç\n",
                ":icon: false\n",
                ":class: simple\n",
                "We moeten de boomstructuur als volgt \"lezen\":\n",
                "  \n",
                "```python\n",
                "if petal_length <= 2.45:\n",
                "    return \"Setosa\"\n",
                "else:\n",
                "    if petal_width <= 1.75:\n",
                "        if petal_length <= 4.95:\n",
                "            if petal_width <= 1.65:\n",
                "                return \"Versicolor\"\n",
                "            else:\n",
                "                return \"Virginica\"\n",
                "        else:\n",
                "            if petal_width <= 1.55:\n",
                "                return \"Virginica\"\n",
                "            else:\n",
                "                if sepal_length <= 6.95:\n",
                "                    return \"Versicolor\"\n",
                "                else:\n",
                "                    return \"Virginica\"\n",
                "    else:\n",
                "        if petal_length <= 4.85:\n",
                "            if sepal_length <= 5.95:\n",
                "                return \"Versicolor\"\n",
                "            else:\n",
                "                return \"Virginica\"\n",
                "        else:\n",
                "            return \"Virginica\"\n",
                ":::"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e81e93d5",
            "metadata": {},
            "source": [
                "## Parameters\n",
                "**De volledige conditionele structuur, zoals hierboven ge√Øllustreerd, vormt de parameters van het model. We weten met andere woorden niet op voorhand met hoeveel parameters we zullen eindigen - of hoe complex het model uiteindelijk zal zijn**. Dat heeft het voordeel dat we een heel flexibel model hebben, maar het nadeel dat we goed moeten opletten voor over-complexiteit en over-fitting. Er zijn wel verschillende hyper parameters zoals de maximum diepte van de vertakkingen, het minimum aantal voorbeelden in een vertakking, enz, maar dat verandert niets aan het feit dat we de model complexiteit niet rechstreeks in de hand hebben."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e751dcbe",
            "metadata": {},
            "outputs": [],
            "source": [
                "clf2 = tree.DecisionTreeClassifier(max_depth=2, random_state=123)\n",
                "clf2 = clf2.fit(X, y)\n",
                "\n",
                "viz_model2 = dtreeviz.model(\n",
                "    clf2,\n",
                "    X_train=X,\n",
                "    y_train=y,\n",
                "    feature_names=iris.feature_names,\n",
                "    target_name=\"Iris type\",\n",
                "    class_names=iris.target_names,\n",
                ")\n",
                "\n",
                "viz_model2.view(orientation=\"LR\", fontname=\"DejaVu Sans Mono\", scale=1.5)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8b56f0f2",
            "metadata": {},
            "source": [
                "## Features\n",
                "_Decision trees_ kunnen algemeen gesproken flexibel om met veel en heterogene features. Ze hebben ook een natuurlijke manier om features met een hoge en lage discriminatieve waarde te onderscheiden. **_Distinctive features_** zijn features met een hoge discriminatieve waarde; hun waarden correleren heel sterk met de target waarden.\n",
                "\n",
                ":::{note} üåç\n",
                ":icon: false\n",
                ":class: simple\n",
                "`Petal length` heeft een hoge _distinctive value_ met betrekking tot het onderscheid `Setosa` versus `{Versicolor, Virginica}`\n",
                ":::\n",
                "\n",
                "Op iedere niveau worden alle features in overweging genomen, maar enkel de meest _informatieve_ wordt gebruikt. Daardoor komen minder informatieve features automatisch naar diepere vertakkingen geduwd en worden niet-informatieve features automatisch geweerd uit het model. Er dus een impliciete feature selectie.\n",
                "  \n",
                "Doordat feature-schalen simpelweg opgedeeld worden binnen de boomstructuur, zijn _decision trees_ ook heel geschikt om niet-lineaire verbanden te herkennen - zonder dat er veel _feature engineering_ aan te pas moet komen.\n",
                "  \n",
                "Tenslotte kunnen _decision trees_ ook vrij gemakkelijk omgaan met _missing values_. De vaakst gebruikte strategie zijn _surrogate splits_. Hierbij wordt voor iedere vertakkingsconditie (bv. `petal_length <= 2.45`) een back-up regel of back-up regels voorzien."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a0382f8b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance scores\n",
                "feature_importance = clf.feature_importances_\n",
                "for name, importance in zip(iris.feature_names, feature_importance, strict=False):\n",
                "    print(f\"{name}: {importance:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "33dbf350",
            "metadata": {},
            "source": [
                "## Optimalisatie\n",
                "Om de optimale vertakkingen te vinden wordt **sequentieel gezocht naar de best mogelijke _split_ op een van de features**. Er worden bij iedere stap volgens bepaalde (deels random) principes een reeks kandidaat _splits_ opgemaakt. Die kandidaten worden gevalueerd met betrekking tot de kwaliteit van de predicties op het volgende niveau. Er is geen algemene _loss_-functie, maar wel op het niveau van individuele splits. In veel tekstboeken wordt niet over een _loss_-functie gesproken in de context van _decision trees_, maar er is wel degelijk sprake van een evaluatie van (sub)optimaliteit tijdens het trainen.\n",
                "  \n",
                "In de context van klassificatie wordt er bij iedere _split_ gekeken naar de _impurity_: hoe zuiver zijn de data met betrekking tot de target categori√´n? _Gini impurity_ en _Entropy_ zijn de meest voorkomende maten. Beiden komen in detail aan bod in de cursus Mathematical Foundations. In het geval van regressie is het de bedoeling dat een _split_ tot een betere predictie leidt in de vertakkingen. Er wordt in plaats van _impurity_ naar de _Mean Squared_ of _Absolute Error (MSE/MAE)_ gekeken. \n",
                "\n",
                "## Taken, Ervaring, Performantie\n",
                "Naast **klassificatie** kunnen _decision trees_ ook voor **regressie** gebruikt worden. In beide gevallen gaat het om **supervised** learning.\n",
                "\n",
                "\n",
                ":::{note} üåç\n",
                ":icon: false\n",
                ":class: simple\n",
                "Om regressie met _decision trees_ te illustreren kijken we naar een dataset waarbij we het verbruik (_Miles Per Gallon_) trachten te voorspellen aan de hand van het aantal cylinders, cylinderinhoud en gewicht.\n",
                ":::"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "373c3d2c",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_url = \"https://raw.githubusercontent.com/parrt/dtreeviz/master/data/cars.csv\"\n",
                "df = pd.read_csv(dataset_url)\n",
                "X = df.drop(\"MPG\", axis=1)\n",
                "y = df[\"MPG\"]\n",
                "features = list(X.columns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ceecd46",
            "metadata": {},
            "outputs": [],
            "source": [
                "reg = tree.DecisionTreeRegressor(max_depth=3, criterion=\"absolute_error\")\n",
                "reg.fit(X.values, y.values)\n",
                "\n",
                "viz_rmodel = dtreeviz.model(reg, X, y, feature_names=features, target_name=\"MPG\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ae86465",
            "metadata": {},
            "outputs": [],
            "source": [
                "viz_rmodel.rtree_feature_space(features=[\"WGT\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2216dc95",
            "metadata": {},
            "outputs": [],
            "source": [
                "viz_rmodel.rtree_feature_space3D(\n",
                "    features=[\"WGT\", \"ENG\"],\n",
                "    fontsize=10,\n",
                "    elev=30,\n",
                "    azim=20,\n",
                "    show={\"splits\", \"title\"},\n",
                "    colors={\"tessellation_alpha\": 0.5},\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c853e2a4",
            "metadata": {},
            "source": [
                "Om de performatie van _decision trees_ te evalueren wordt in de eerste plaats gekeken naar de metrieken die ook de training sturen (_Gini impurity/Entropy/Misclassification rate/MSE/MAR_).\n",
                "Omdat er een re√´el gevaar bestaat voor _overfitting_ wordt veel belang gehecht aan de performantie bij ongeziene test data."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aed25d35",
            "metadata": {},
            "source": [
                "## Voordelen\n",
                "- Interpreteerbaarheid: Iedere vertakking heeft een duidelijke betekenis\n",
                "- Flexibiliteit\n",
                "- Computationele effici√´ntie omdat er geen globale optimalisatie gebeurt (_greedy_ optimisatie)\n",
                "\n",
                "## Nadelen\n",
                "- _Greedy_ optimalisatie geeft geen garantie op een globaal optimale boomstructuur\n",
                "- Onstabiliteit: kleine aanpassingen aan de data kunnen een groot verschil geven in de boomstructuur\n",
                "- **Overfitting**: het aantal parameters kan ongecontroleerd groeien waardoor er overfitting optreedt"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml-courses",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
