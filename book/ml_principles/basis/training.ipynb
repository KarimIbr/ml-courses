{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca8e2f9",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Zoals eerder vermeld, zijn er twee basis stappen in machine learning:\n",
    "1. Model training\n",
    "2. Model inference\n",
    "\n",
    "Op het moment dat we een model hebben gedefini√´erd, willen we de parameters aan de hand van de beschikbare data schatten. Dat doen we door het gepaste leer-algorithme toe te passen.\n",
    "Dit noemen we **model training**.  \n",
    "  \n",
    "Een **getrained model is een model waarvan de parameters geschat zijn met behulp van een wel bepaalde traing dataset en optimalisatie-algorithme**.\n",
    "\n",
    "Pas als het model getrained is, kan het gebruikt worden om effectief patroonherkenning te doen op _nieuwe data_. Die stap heet _model inference_.\n",
    "\n",
    ":::{warning}\n",
    "Het is niet omdat we een getraind model hebben, dat dit automatisch betekent dat het in staat is om de patronen te herkennen waar we naar op zoek zijn!  \n",
    "**Onthou**: _All models are wrong, some are useful_.\n",
    ":::\n",
    "\n",
    "### Optimalisatie-algorithme\n",
    "Een optimalisatie-algorithme heeft als doel om de optimale waarde voor een parameter te vinden, gegeven ons model (van de werkelijkheid) en de beschikbare data.\n",
    "_Bij het uitvoeren van het optimalisatie-algorithme gebeurt dus het eigenlijke leren in ML_. Ook hier zijn vaak verschillende keuzes aan de orde.\n",
    "Daarbij moeten we (a) effectiviteit om juiste parameterwaarden te vinden en (b) computationele efficientie tegen elkaar afgewegen.\n",
    "\n",
    ":::{note} üåç\n",
    ":icon: false\n",
    ":class: simple \n",
    "In ons eennvoudige airco voorbeeld kozen we voor het sample gemiddelde.\n",
    ":::\n",
    "\n",
    "De keuze van het algorithme hangt samen met het type model (lineair, neuraal netwerk, random forest, enz.) en, hoewel er voortdurend verder onderzoek gebeurd naar nieuwe technieken, zijn voor veel modellen geijkte keuzes beschikbaar. \n",
    "Zoals we in de uitgebreidere voorbeelden ook zullen zien, zijn leer- of optimalisatiealgorithmes vaak zeer complex (denk alvast aan het [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) algoritme bij neurale netwerken en uitgebreid aan bod komt in de cursus Mathematical Foundations).  \n",
    "In die complexiteit zit vaak ook nog het feit dat er verdere keuzes moeten gemaakt worden met betrekking tot de _specifieke configuratie van het algorithme zelf_.\n",
    "Bij iteratieve methodes, bijvoorbeeld, waarin er stapsgewijs gezocht wordt naar een optimale waarde, moeten we een keuze maken over de grootte van die stappen (de zogenaamde _learning rate_).  \n",
    "Die configuratie kan een grote impact hebben op de kwaliteit van het uiteindelijke resultaat. Het is vaak op voorhand ook niet duidelijk welke de beste configuratie voor het optimalisatie algorithme is, gegeven de specifieke situatie. Daarom wordt ook daar vaak op een (al dan niet principi√´le manier) iteratief gewerkt om tot optimale waarden te komen.  \n",
    "**De numerieke configuratie parameters van een optimalisatiealgorithme worden _hyper parameters_ genoemd. Het process waarbij de hyper parameters worden geoptimaliseerd heeft _hyper parameter tuning_.**\n",
    "\n",
    "### Supervisie\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
